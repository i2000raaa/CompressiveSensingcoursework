{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7495f523",
      "metadata": {
        "id": "7495f523"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from numpy import linalg\n",
        "from scipy import sparse\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from math import sqrt, log\n",
        "import statistics\n",
        "import scipy.linalg\n",
        "import copy\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63767059-0485-4e53-97e5-d8b3c8bfe0cd",
      "metadata": {
        "id": "63767059-0485-4e53-97e5-d8b3c8bfe0cd",
        "outputId": "a5ab35d5-b1a9-4909-b514-175b37766d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "n_available_cores = len(os.sched_getaffinity(0))\n",
        "print(n_available_cores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Данные umich rss\n",
        "https://doi.org/10.15783/C7R30H\n"
      ],
      "metadata": {
        "id": "ID-28jlh-oaw"
      },
      "id": "ID-28jlh-oaw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0250e1d5",
      "metadata": {
        "id": "0250e1d5"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with open(\"/content/sample_data/Z.txt\") as f:\n",
        "    for line in f:\n",
        "        data.append([str(x) for x in line.split(sep='/n')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202ff425",
      "metadata": {
        "id": "202ff425"
      },
      "outputs": [],
      "source": [
        "a = []\n",
        "for i in range(len(data)):\n",
        "    a.append(list([float(x) for x in data[i][0].split(sep=',')]))\n",
        "d = np.matrix(np.array(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Данные cu rssi\n",
        "https://dx.doi.org/10.15783/C7G594\n"
      ],
      "metadata": {
        "id": "TCXL1YAb_HWi"
      },
      "id": "TCXL1YAb_HWi"
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/sample_data/omni_16dbm (1).txt'\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        data.append([str(x).replace(' ' , ',').replace('\\n' , '') for x in line.split(sep='/n')])"
      ],
      "metadata": {
        "id": "37YmRgY7_POl"
      },
      "id": "37YmRgY7_POl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "for i in range(len(data)):\n",
        "    a.append(list([x for x in data[i][0].split(sep=',')]))\n",
        "\n",
        "Y = np.array(a)"
      ],
      "metadata": {
        "id": "kEpa4V4F_PRJ"
      },
      "id": "kEpa4V4F_PRJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ = pd.DataFrame(d)\n",
        "# frame 500 location 895\n",
        "df_ = df_.rename(columns={0:'x', 1:'y', 5:'frame'})\n",
        "df_['x'] = np.round(df_['x'].astype(float), 4)\n",
        "df_['y'] = np.round(df_['y'].astype(float), 4)\n",
        "df_[4] = df_[4].astype(float)\n",
        "df_['frame'] = df_['frame'].astype(float)\n",
        "df_[6] = df_[6].astype(float)\n",
        "df_[7] = df_[7].astype(float)\n",
        "df_[8] = df_[8].astype(float)\n",
        "df_[9] = df_[9].astype(float)\n",
        "df_[10] = df_[10].astype(float)\n",
        "\n",
        "df_ = df_.reset_index()"
      ],
      "metadata": {
        "id": "_TmrUDp6_PTl"
      },
      "id": "_TmrUDp6_PTl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.DataFrame({\n",
        "    'x': df_['x'].repeat(5),\n",
        "    'y': df_['y'].repeat(5),\n",
        "    'frame' : df_['frame'].repeat(5),\n",
        "    'type': df_.columns[7:].tolist() * (len(df_)),\n",
        "    'value': df_.iloc[:, 7:].values.flatten()\n",
        "})"
      ],
      "metadata": {
        "id": "qdW5933M_PVy"
      },
      "id": "qdW5933M_PVy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.pivot_table(df_new, values='value', index=['x', 'y', 'type'],\n",
        "                       columns=['frame'], aggfunc='mean').reset_index().dropna().drop(columns=['x', 'y', 'type'])\n",
        "print(df.shape)\n",
        "Y = df.values"
      ],
      "metadata": {
        "id": "sHbAz2PZ_iT-"
      },
      "id": "sHbAz2PZ_iT-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = np.matrix(Y)"
      ],
      "metadata": {
        "id": "SjP14eBi_pi2"
      },
      "id": "SjP14eBi_pi2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "be0f9479",
      "metadata": {
        "id": "be0f9479"
      },
      "source": [
        "Применим разложение по сингулярным значениям (SVD), чтобы проверить, имеет ли центрированная матрица хорошую аппроксимацию низкого ранга."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_means = np.mean(d, axis=1)\n",
        "d = d - row_means\n",
        "\n",
        "shape0 = d.shape[0]\n",
        "shape1 = d.shape[1]\n",
        "print(shape0, shape1)"
      ],
      "metadata": {
        "id": "EJvtdxIJ_x5h"
      },
      "id": "EJvtdxIJ_x5h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83389d1f",
      "metadata": {
        "id": "83389d1f"
      },
      "outputs": [],
      "source": [
        "main_arr = np.array(d)\n",
        "u, s, vt = np.linalg.svd(main_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5256f190",
      "metadata": {
        "id": "5256f190",
        "outputId": "3063d793-26bb-4773-ff9c-37d5051ce80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32.4 % singular values to capture 90 % variance\n"
          ]
        }
      ],
      "source": [
        "s2=[]\n",
        "s2sum=0\n",
        "for i in range(0, s.shape[0]):\n",
        "    s2.append(s[i]**2)\n",
        "    s2sum=s2sum+s[i]**2\n",
        "\n",
        "s2matr=np.array(s2)\n",
        "\n",
        "total_variance = 0\n",
        "num_sv = 0\n",
        "for i in range(0, s.shape[0]):\n",
        "    if total_variance < 0.9:\n",
        "        total_variance += s2matr[i]/s2sum\n",
        "        num_sv += 1\n",
        "print(round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db69e7d",
      "metadata": {
        "id": "9db69e7d"
      },
      "source": [
        "Введём аномалии, чтобы увидеть, как это влияет на результаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11515f1",
      "metadata": {
        "id": "d11515f1",
        "outputId": "7a55f4f8-770c-4b62-c035-cfd13b45e784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anomoly_size 14.753346168551886\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df = pd.DataFrame(d)\n",
        "#используем экспоненциально-взвешенную скользящую среднюю (EWMA) для прогнозирования будущих записей на основе их прошлых значений\n",
        "df_ewm = df.ewm(alpha=0.8, adjust=False).mean()\n",
        "#используем максимальную разницу между фактическим и прогнозируемым значением в качестве размера вводимой аномалии\n",
        "dt = df - df_ewm\n",
        "max_by_columns = dt.max()\n",
        "anomoly_size = max(max_by_columns)\n",
        "print(\"Anomoly_size\", anomoly_size)\n",
        "\n",
        "#изменим долю записей для введения аномалий с 5% до 10%, а также масштабируем размер аномалии на s, который равен 0,5 или 1.\n",
        "s_5 = anomoly_size*0.5\n",
        "s_1 = anomoly_size*1\n",
        "\n",
        "random_indx_y10 = np.random.randint(0, 181, size=round(182*3127*0.1))\n",
        "random_indx_x10 = np.random.randint(0, 3126, size=round(182*3127*0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ab19aa",
      "metadata": {
        "id": "26ab19aa",
        "outputId": "4e98285f-d880-4ff9-ec75-006bb62f515f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With s = 0.5, fraction = 10%  64.3 % singular values to capture 90 % variance\n"
          ]
        }
      ],
      "source": [
        "df_ = df.copy(deep=True)\n",
        "for i in range(len(random_indx_y10)):\n",
        "    y = int(random_indx_y10[i])\n",
        "    x = int(random_indx_x10[i])\n",
        "    df_.iat[y, x] =  df_.at[y, x] + s_5\n",
        "\n",
        "df_matrix = np.asmatrix(df_)\n",
        "u, s, vt = np.linalg.svd(df_matrix)\n",
        "\n",
        "s2=[]\n",
        "s2sum=0\n",
        "for i in range(0, s.shape[0]):\n",
        "    s2.append(s[i]**2)\n",
        "    s2sum=s2sum+s[i]**2\n",
        "\n",
        "s2matr=np.array(s2)\n",
        "total_variance = 0\n",
        "num_sv = 0\n",
        "for i in range(0, s.shape[0]):\n",
        "    if total_variance < 0.9:\n",
        "        total_variance += s2matr[i]/s2sum\n",
        "        num_sv += 1\n",
        "\n",
        "print(\"With s = 0.5, fraction = 10% \", round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75f715c",
      "metadata": {
        "id": "f75f715c",
        "outputId": "8b3ac8ca-a3c9-4de4-d19a-f9bf4cdda890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With s = 0.5, fraction = 5%  53.300000000000004 % singular values to capture 90 % variance\n"
          ]
        }
      ],
      "source": [
        "df_ = df.copy(deep=True)\n",
        "for i in range(len(random_indx_y10)//2):\n",
        "    y = int(random_indx_y10[i])\n",
        "    x = int(random_indx_x10[i])\n",
        "    df_.iat[y, x] =  df_.at[y, x] + s_5\n",
        "\n",
        "df_matrix = np.asmatrix(df_)\n",
        "u, s, vt = np.linalg.svd(df_matrix)\n",
        "\n",
        "s2=[]\n",
        "s2sum=0\n",
        "for i in range(0, s.shape[0]):\n",
        "    s2.append(s[i]**2)\n",
        "    s2sum=s2sum+s[i]**2\n",
        "\n",
        "s2matr=np.array(s2)\n",
        "total_variance = 0\n",
        "num_sv = 0\n",
        "for i in range(0, s.shape[0]):\n",
        "    if total_variance < 0.9:\n",
        "        total_variance += s2matr[i]/s2sum\n",
        "        num_sv += 1\n",
        "\n",
        "print(\"With s = 0.5, fraction = 5% \", round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad07a41",
      "metadata": {
        "id": "8ad07a41",
        "outputId": "020455cd-126c-4628-f342-5544dd14d737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With s = 1, fraction = 10%  78.60000000000001 % singular values to capture 90 % variance\n"
          ]
        }
      ],
      "source": [
        "df_ = df.copy(deep=True)\n",
        "for i in range(len(random_indx_y10)):\n",
        "    y = int(random_indx_y10[i])\n",
        "    x = int(random_indx_x10[i])\n",
        "    df_.iat[y, x] =  df_.at[y, x] + s_1\n",
        "\n",
        "df_matrix = np.asmatrix(df_)\n",
        "u, s, vt = np.linalg.svd(df_matrix)\n",
        "\n",
        "s2=[]\n",
        "s2sum=0\n",
        "for i in range(0, s.shape[0]):\n",
        "    s2.append(s[i]**2)\n",
        "    s2sum=s2sum+s[i]**2\n",
        "\n",
        "s2matr=np.array(s2)\n",
        "total_variance = 0\n",
        "num_sv = 0\n",
        "for i in range(0, s.shape[0]):\n",
        "    if total_variance < 0.9:\n",
        "        total_variance += s2matr[i]/s2sum\n",
        "        num_sv += 1\n",
        "print(\"With s = 1, fraction = 10% \", round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d20eaa5",
      "metadata": {
        "id": "9d20eaa5",
        "outputId": "a1c595a5-0b90-47e0-b809-86f732a64181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With s = 1, fraction = 5%  73.6 % singular values to capture 90 % variance\n"
          ]
        }
      ],
      "source": [
        "df_ = df.copy(deep=True)\n",
        "for i in range(len(random_indx_y10)//2):\n",
        "    y = int(random_indx_y10[i])\n",
        "    x = int(random_indx_x10[i])\n",
        "    df_.iat[y, x] =  df_.at[y, x] + s_1\n",
        "\n",
        "df_matrix = np.asmatrix(df_)\n",
        "u, s, vt = np.linalg.svd(df_matrix)\n",
        "\n",
        "s2=[]\n",
        "s2sum=0\n",
        "for i in range(0, s.shape[0]):\n",
        "    s2.append(s[i]**2)\n",
        "    s2sum=s2sum+s[i]**2\n",
        "\n",
        "s2matr=np.array(s2)\n",
        "total_variance = 0\n",
        "num_sv = 0\n",
        "for i in range(0, s.shape[0]):\n",
        "    if total_variance < 0.9:\n",
        "        total_variance += s2matr[i]/s2sum\n",
        "        num_sv += 1\n",
        "\n",
        "print(\"With s = 1, fraction = 5% \", round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "535b175d",
      "metadata": {
        "id": "535b175d",
        "outputId": "d4313f16-6534-40f3-c8a5-74f0a5dd6813"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "134"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_sv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = np.matrix(Y)\n",
        "shape0 = d.shape[0]\n",
        "shape1 = d.shape[1]\n",
        "print(shape0, shape1)\n",
        "main_arr = np.array(d)\n",
        "df = pd.DataFrame(d)"
      ],
      "metadata": {
        "id": "UzWkPqHt_6Y1"
      },
      "id": "UzWkPqHt_6Y1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "035b4800",
      "metadata": {
        "id": "035b4800"
      },
      "source": [
        "Чем вводится больше аномалий, или чем они больше по размеру, тем больший процент сингулярных чисел необходим для объяснения 90% вариативности данных."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84fbad0b",
      "metadata": {
        "id": "84fbad0b"
      },
      "source": [
        "### LENS Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ec553e5",
      "metadata": {
        "id": "5ec553e5"
      },
      "outputs": [],
      "source": [
        "def soft_thresh(x, T):\n",
        "    \"\"\"Soft threshold function\"\"\"\n",
        "    return  np.multiply(np.sign(x), np.maximum(np.abs(x) - T, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dccdf70-f655-4bab-a1dc-3c4151476428",
      "metadata": {
        "id": "0dccdf70-f655-4bab-a1dc-3c4151476428"
      },
      "outputs": [],
      "source": [
        "# new\n",
        "def ADMM(D, frac_anomaly, frac_missing,m=shape0, n=shape1):\n",
        "\n",
        "    X = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(n, n))\n",
        "    Y = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(m, n))\n",
        "    Z = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(m, n))\n",
        "    A = np.ones((shape0, shape1))\n",
        "    B = np.ones((shape0, shape0))\n",
        "    C = np.eye(shape0)\n",
        "\n",
        "    X_0 = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(n, n))\n",
        "    X_1 = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(n, n))\n",
        "    Y_0 = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(m, n))\n",
        "    M_0 = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(n, n))\n",
        "    M_1 = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(n, n))\n",
        "    N = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(m, n))\n",
        "    M = np.random.randint(int(np.nanmin(D)), int(np.nanmax(D)), size=(m, n))\n",
        "    non_elem = np.argwhere(D != None)\n",
        "    D[np.isnan(D)] = 0\n",
        "\n",
        "    E = np.array([np.where(x == None, 1, 0) for x in D])\n",
        "    W = D - np.matmul(A, X) - np.matmul(B, Y) - np.matmul(C, Z)\n",
        "    for g in range(len(non_elem)):\n",
        "        W[non_elem[g][0]][non_elem[g][1]] = 0\n",
        "    shape_d = (m, n)    #(shape0, shape1)\n",
        "    shape_X = shape_Y = shape_d\n",
        "    size_d = m*n\n",
        "    #nu_D - это доля записей в D, которые не являются ни пропущенными, ни ошибочными.\n",
        "    nu_D = 1 - (frac_missing+frac_anomaly)/size_d\n",
        "\n",
        "    mu = 1.01\n",
        "    p = 1.05\n",
        "\n",
        "    J = D - A@X_0 - B@Y_0 - W\n",
        "    alpha = (shape_X[0]**(0.5) + shape_X[1]**(0.5))*nu_D\n",
        "    beta = (2*math.log(shape_Y[0]*shape_Y[1]))**(0.5)\n",
        "    gamma = 1\n",
        "    theta = 10\n",
        "    K = 1\n",
        "    P_1 = np.eye(n)\n",
        "    q1_list_1 = [1] +  [0] * (n-1)\n",
        "    q1_list_2 = [1, -1] + [0] * (n-2)\n",
        "    Q_1 =  scipy.linalg.toeplitz(q1_list_1, q1_list_2)\n",
        "    R_1 = np.zeros(n)\n",
        "\n",
        "    #sigma_D определяется на каждой иттерации ADM алгоритма\n",
        "    #как стандартное отклонение ряда значений J[i,j], где E[i,j]=0\n",
        "    tmp_nonzero = []\n",
        "    indices = np.nonzero(~np.isnan(E)) # выбираем индексы, где в Е стоят 0 - то есть где не было пропуск или ошибки\n",
        "    non_elem = np.column_stack(indices)\n",
        "    for h in range(len(non_elem)):\n",
        "        tmp_nonzero.append(J[non_elem[h][0]][non_elem[h][1]])\n",
        "    sigma_D = statistics.stdev(tmp_nonzero)\n",
        "    sigma = theta*sigma_D\n",
        "    MAX_ITER = 400\n",
        "    j = 0\n",
        "    for i in tqdm(range(MAX_ITER)):\n",
        "        j += 1\n",
        "        #print(\"Iteration\", i)\n",
        "\n",
        "        #first step X\n",
        "        J = 1/(K+1)*(X_1 + M_1/mu + X_0 + M_0/mu)\n",
        "        t = alpha/(mu*(K+1))\n",
        "        u, s, vt = scipy.linalg.svd(J, lapack_driver='gesvd')\n",
        "        S = np.diag(s)\n",
        "        s = soft_thresh(S, t)\n",
        "        s = np.diag(s)\n",
        "        X = u[:,:2].dot(np.diag(s[:2])).dot(vt[:2,:])\n",
        "\n",
        "        #second step\n",
        "        J = X - M_1/mu\n",
        "        R = P_1.transpose() @ R_1 @ Q_1 + ((mu*sigma)/gamma) * J\n",
        "        up, sp, vtp = np.linalg.svd(P_1.transpose() @ P_1)\n",
        "        uq, sq, vtq = np.linalg.svd(Q_1.transpose() @ Q_1)\n",
        "        X_1 = up @ np.divide((vtp @ R @ uq), (sp*sq.transpose() + (mu*sigma/gamma))) @ vtq\n",
        "\n",
        "        #third step with X0\n",
        "        J_0 = X-M_0/mu\n",
        "        J = D- B@Y_0 - C@Z - W + M/mu\n",
        "        X_0 = np.linalg.inv(A.transpose()@A + np.eye(n))@(A.transpose()@J + J_0)\n",
        "\n",
        "        #fourth step Y\n",
        "        J = Y_0 + N/mu\n",
        "        t = beta/mu\n",
        "        Y = soft_thresh(J, t)\n",
        "\n",
        "        #fifth step Y_0\n",
        "        J_0 = Y - N/mu\n",
        "        J = D - A@X_0 - C@Z - W + M/mu\n",
        "        Y_0 = np.linalg.inv(B.transpose()@B + np.eye(m))@(B.transpose()@J + J_0)\n",
        "\n",
        "        #sixth step Z\n",
        "        J = D-np.matmul(A, X_0)-np.matmul(B, Y_0)-W + M/mu\n",
        "        Z = np.linalg.inv(1/(mu*sigma)*np.eye(m) + C.transpose()@C)@(C.transpose()@J)\n",
        "\n",
        "        #seventh step W\n",
        "        W = np.multiply(E, (D - A@X_0 - B@Y_0 - C@Z + M/mu))\n",
        "\n",
        "        #eighth step sigma\n",
        "        theta = 10\n",
        "        J = D - A@X_0 - B@Y_0 - W\n",
        "        tmp_nonzero = []\n",
        "        indices = np.nonzero(~np.isnan(E))\n",
        "        non_elem = np.column_stack(indices)\n",
        "        for z in range(len(non_elem)):\n",
        "            tmp_nonzero.append(J[non_elem[z][0]][non_elem[z][1]])\n",
        "\n",
        "        sigma_D = statistics.stdev(tmp_nonzero)\n",
        "        sigma = theta*sigma_D\n",
        "\n",
        "        #ninth step\n",
        "        #Every itteration update M, M_0, N\n",
        "        M = M + mu * (D - A@X_0 - B@Y_0 - C@Z - W)\n",
        "        M_0 = M_0 + mu * (X_0-X)\n",
        "        M_1 = M_1 + mu*(X_1-X)\n",
        "        N = N +  mu * (Y_0-Y)\n",
        "\n",
        "        #tenth step\n",
        "        #Every itteration update mu\n",
        "        mu = p*mu\n",
        "        #Every 100 iterations, we multiply ρ by 1.05\n",
        "        if j%100 == 0:\n",
        "            p_ = 1.05\n",
        "            p = p*p_\n",
        "        if sigma_D == 0:\n",
        "            return D\n",
        "        D = A @ X_0 + B @ Y_0 + C @ Z + W\n",
        "    return D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0ef7b1-740e-4793-b84f-6d594a08941a",
      "metadata": {
        "id": "8e0ef7b1-740e-4793-b84f-6d594a08941a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import copy\n",
        "import random\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.impute import KNNImputer\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "\n",
        "#ВЫБРАТЬ ТЕХНИКИ:\n",
        "miss_technique = 3\n",
        "anomal_technique = 2\n",
        "\n",
        "# НАСТРОЕНЫ:\n",
        "df_ = df.copy(deep=True)\n",
        "D = np.asmatrix(df_)\n",
        "tmp_arr = copy.deepcopy(main_arr)\n",
        "\n",
        "new_loss_result_list = []\n",
        "new_loss_result_list_2 = []\n",
        "new_loss_result_list_3 = []\n",
        "\n",
        "p_ = [0.2, 0.4, 0.8]\n",
        "anomaly_size = [0.5, 1, 2, 5] #относительная амплитуда вводимых аномалий\n",
        "anomaly_ratio = [0, 0.05, 0.1, 0.2, 0.4] #доля вводимых аномалий\n",
        "loss_probability = [0.2, 0.4, 0.8] #доля вводимых пропусков\n",
        "loss = 0.2\n",
        "anomaly_r = 0.1\n",
        "anomaly_s = 1\n",
        "\n",
        "\n",
        "#for p in p_:\n",
        "for anomaly_r in anomaly_ratio:\n",
        "#for anomaly_s in anomaly_size:\n",
        "# for loss in loss_probability:\n",
        "    new_nmae_err_list = []\n",
        "    new_nmae_err_list_2 = []\n",
        "    new_nmae_err_list_3 = []\n",
        "    nmae_err_list = []\n",
        "    nmae_err_list_2 = []\n",
        "    nmae_err_list_3 = []\n",
        "    frac_missing = round(shape0*shape1*loss)\n",
        "    frac_anomaly = round(shape0*shape1*anomaly_r)//2\n",
        "    for i in range(10):\n",
        "        if miss_technique == 1:\n",
        "            random_miss_pairs = [(random.randint(0, shape0-1), random.randint(0, shape1-1)) for _ in range(round(shape0*shape1*loss))]\n",
        "        elif miss_technique == 2:\n",
        "            random_indx_y10_miss = np.array([i for i in range(shape0)])\n",
        "            random_indx_x10_miss = np.random.randint(0, shape1-1, size=round(shape1*loss))\n",
        "        elif miss_technique == 3:\n",
        "            random_indx_y10_miss = np.random.randint(0, shape0-1, size=round(shape0*loss))\n",
        "            random_indx_x10_miss = np.array([i for i in range(shape1)])\n",
        "\n",
        "\n",
        "        list_D = []\n",
        "        list_result_D = []\n",
        "        list_result_D_2 = []\n",
        "        list_result_D_3 = []\n",
        "\n",
        "        tmp_arr = copy.deepcopy(main_arr)\n",
        "        tmp_arr_for_svd = copy.deepcopy(tmp_arr)\n",
        "\n",
        "        if anomal_technique == 1:\n",
        "            random_anomal_pairs = [(random.randint(0, shape0), random.randint(0, shape1)) for _ in range(shape0*shape1*anomaly_r)]\n",
        "            for i in range(len(random_anomal_pairs)):\n",
        "                y = int(random_anomal_pairs[i][0])\n",
        "                x = int(random_anomal_pairs[i][1])\n",
        "                tmp_arr[y][x] += s_1\n",
        "                tmp_arr_for_svd[y][x] += s_1\n",
        "        else:\n",
        "            random_indx_y10_anomal = np.array([i for i in range(shape0)])\n",
        "            random_indx_x10_anomal = np.random.randint(0, shape1-1, size=round(shape1*anomaly_r))\n",
        "            for i in range(len(random_indx_y10_anomal)):\n",
        "                for j in range(len(random_indx_x10_anomal)):\n",
        "                    y = int(random_indx_y10_anomal[i])\n",
        "                    x = int(random_indx_x10_anomal[j])\n",
        "                    tmp_arr[y][x] += s_1\n",
        "                    tmp_arr_for_svd[y][x] += s_1\n",
        "\n",
        "# for first technique\n",
        "        if miss_technique == 1:\n",
        "            for i in range(len(random_miss_pairs)):\n",
        "                y = int(random_miss_pairs[i][0])\n",
        "                x = int(random_miss_pairs[i][1])\n",
        "                list_D.append(tmp_arr[y][x])\n",
        "            for i in range(len(random_miss_pairs)):\n",
        "                y = int(random_miss_pairs[i][0])\n",
        "                x = int(random_miss_pairs[i][1])\n",
        "                tmp_arr[y][x] = None\n",
        "# for second, third techniques\n",
        "        else:\n",
        "            for i in range(len(random_indx_y10_miss)):\n",
        "                for j in range(len(random_indx_x10_miss)):\n",
        "                    y = int(random_indx_y10_miss[i])\n",
        "                    x = int(random_indx_x10_miss[j])\n",
        "                    list_D.append(tmp_arr[y][x])\n",
        "\n",
        "            for i in range(len(random_indx_y10_miss)):\n",
        "                for j in range(len(random_indx_x10_miss)):\n",
        "                    y = int(random_indx_y10_miss[i])\n",
        "                    x = int(random_indx_x10_miss[j])\n",
        "                    tmp_arr[y][x] = None\n",
        "\n",
        "        mean = np.nanmean(tmp_arr)\n",
        "        tmp_arr_for_svd[np.isnan(tmp_arr_for_svd)] = 0\n",
        "        indices = np.nonzero(np.isnan(tmp_arr_for_svd))\n",
        "\n",
        "        U, sigma, V = np.linalg.svd(tmp_arr_for_svd)\n",
        "        U = U[:, :2]\n",
        "        sigma = np.diag(sigma[:2])\n",
        "        V = V[:2, :]\n",
        "        # Вычислим аппроксимированную матрицу\n",
        "        D_approx = U @ sigma @ V\n",
        "\n",
        "        # Применяем K ближайших соседей для локальной интерполяции:\n",
        "        knn = KNeighborsRegressor(n_neighbors=2)\n",
        "        matrix_interp = knn.fit(D_approx, main_arr).predict(D_approx)\n",
        "\n",
        "        # Получаем итоговую матрицу, заменяя пропущенные значения после локальной интерполяции:\n",
        "        D_svd_knn = copy.deepcopy(tmp_arr)\n",
        "        D_svd_knn[np.isnan(D_svd_knn)] = matrix_interp[np.isnan(D_svd_knn)]\n",
        "\n",
        "# для первой техники\n",
        "        if miss_technique == 1:\n",
        "            for i in range(len(random_miss_pairs)):\n",
        "                y = int(random_miss_pairs[i][0])\n",
        "                x = int(random_miss_pairs[i][1])\n",
        "                list_result_D_3.append(D_svd_knn[y][x])\n",
        "                list_result_D_2.append(D_approx[y][x])\n",
        "# для второй и третьей техник\n",
        "        else:\n",
        "            for i in range(len(random_indx_y10_miss)):\n",
        "                for j in range(len(random_indx_x10_miss)):\n",
        "                    y = int(random_indx_y10_miss[i])\n",
        "                    x = int(random_indx_x10_miss[j])\n",
        "                    list_result_D_3.append(D_svd_knn[y][x])\n",
        "                    list_result_D_2.append(D_approx[y][x])\n",
        "\n",
        "        m=shape0\n",
        "        n=shape1\n",
        "        result_D = ADMM(tmp_arr, frac_anomaly, frac_missing, m, n)\n",
        "\n",
        "# для первой техники\n",
        "        if miss_technique == 1:\n",
        "            for i in range(len(random_miss_pairs)):\n",
        "                y = int(random_miss_pairs[i][0])\n",
        "                x = int(random_miss_pairs[i][1])\n",
        "                list_result_D.append(result_D[y][x])\n",
        "# для второй и третьей техник\n",
        "        else:\n",
        "            for i in range(len(random_indx_y10_miss)):\n",
        "                for j in range(len(random_indx_x10_miss)):\n",
        "                    y = int(random_indx_y10_miss[i])\n",
        "                    x = int(random_indx_x10_miss[j])\n",
        "                    list_result_D.append(result_D[y][x])\n",
        "        # расчет метрики ошибки NMAE\n",
        "        raznost = [abs(x - y) for x, y in zip(list_D, list_result_D)]\n",
        "        raznost_2 = [abs(x - y) for x, y in zip(list_D, list_result_D_2)]\n",
        "        raznost_3 = [abs(x - y) for x, y in zip(list_D, list_result_D_3)]\n",
        "        new_nmae_err_list.append(np.sum(raznost) / np.sum(np.abs(list_D)))\n",
        "        new_nmae_err_list_2.append(np.sum(raznost_2) / np.sum(np.abs(list_D)))\n",
        "        new_nmae_err_list_3.append(np.sum(raznost_3) / np.sum(np.abs(list_D)))\n",
        "\n",
        "    new_loss_result_list.append(np.mean(new_nmae_err_list))\n",
        "    new_loss_result_list_2.append(np.mean(new_nmae_err_list_2))\n",
        "    new_loss_result_list_3.append(np.mean(new_nmae_err_list_3))\n",
        "\n",
        "print(new_loss_result_list)\n",
        "print(new_loss_result_list_2)\n",
        "print(new_loss_result_list_3)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Basic Python Environment",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}