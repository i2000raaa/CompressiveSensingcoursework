{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd206d6f",
   "metadata": {},
   "source": [
    "#### Using UMich RSS data\n",
    "https://ieee-dataport.org/open-access/crawdad-umichrss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7495f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import linalg\n",
    "from scipy import sparse\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import sqrt, log\n",
    "import statistics\n",
    "import scipy.linalg\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63767059-0485-4e53-97e5-d8b3c8bfe0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "n_available_cores = len(os.sched_getaffinity(0))\n",
    "print(n_available_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0250e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"Z.txt\") as f:\n",
    "    for line in f:\n",
    "        data.append([str(x) for x in line.split(sep='/n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202ff425",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(len(data)):\n",
    "    a.append(list([float(x) for x in data[i][0].split(sep=',')]))\n",
    "d = np.matrix(np.array(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02673e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be0f9479",
   "metadata": {},
   "source": [
    "Применим разложение по сингулярным значениям (SVD), чтобы проверить, имеет ли центрированная матрица хорошую аппроксимацию низкого ранга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83389d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_arr = np.array(d)\n",
    "u, s, vt = np.linalg.svd(main_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5256f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.4 % singular values to capture 90 % variance\n"
     ]
    }
   ],
   "source": [
    "s2=[]\n",
    "s2sum=0\n",
    "for i in range(0, s.shape[0]):\n",
    "    s2.append(s[i]**2)\n",
    "    s2sum=s2sum+s[i]**2\n",
    "    \n",
    "s2matr=np.array(s2)    \n",
    "\n",
    "total_variance = 0\n",
    "num_sv = 0\n",
    "for i in range(0, s.shape[0]):\n",
    "    if total_variance < 0.9:\n",
    "        total_variance += s2matr[i]/s2sum\n",
    "        num_sv += 1\n",
    "print(round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9db69e7d",
   "metadata": {},
   "source": [
    "Введём аномалии, чтобы увидеть, как это влияет на результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11515f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomoly_size 14.753346168551886\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(d)\n",
    "#используем экспоненциально-взвешенную скользящую среднюю (EWMA) для прогнозирования будущих записей на основе их прошлых значений\n",
    "df_ewm = df.ewm(alpha=0.8, adjust=False).mean()\n",
    "#используем максимальную разницу между фактическим и прогнозируемым значением в качестве размера вводимой аномалии\n",
    "dt = df - df_ewm\n",
    "max_by_columns = dt.max()\n",
    "anomoly_size = max(max_by_columns)\n",
    "print(\"Anomoly_size\", anomoly_size)\n",
    "\n",
    "#изменим долю записей для введения аномалий с 5% до 10%, а также масштабируем размер аномалии на s, который равен 0,5 или 1.\n",
    "s_5 = anomoly_size*0.5\n",
    "s_1 = anomoly_size*1\n",
    "\n",
    "random_indx_y10 = np.random.randint(0, 181, size=round(182*3127*0.1))\n",
    "random_indx_x10 = np.random.randint(0, 3126, size=round(182*3127*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ab19aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With s = 0.5, fraction = 10%  64.3 % singular values to capture 90 % variance\n"
     ]
    }
   ],
   "source": [
    "df_ = df.copy(deep=True)\n",
    "for i in range(len(random_indx_y10)):\n",
    "    y = int(random_indx_y10[i])\n",
    "    x = int(random_indx_x10[i])\n",
    "    df_.iat[y, x] =  df_.at[y, x] + s_5\n",
    "\n",
    "df_matrix = np.asmatrix(df_)\n",
    "u, s, vt = np.linalg.svd(df_matrix)\n",
    "\n",
    "s2=[]\n",
    "s2sum=0\n",
    "for i in range(0, s.shape[0]):\n",
    "    s2.append(s[i]**2)\n",
    "    s2sum=s2sum+s[i]**2\n",
    "    \n",
    "s2matr=np.array(s2)    \n",
    "total_variance = 0\n",
    "num_sv = 0\n",
    "for i in range(0, s.shape[0]):\n",
    "    if total_variance < 0.9:\n",
    "        total_variance += s2matr[i]/s2sum\n",
    "        num_sv += 1\n",
    "\n",
    "print(\"With s = 0.5, fraction = 10% \", round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f75f715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With s = 0.5, fraction = 5%  53.300000000000004 % singular values to capture 90 % variance\n"
     ]
    }
   ],
   "source": [
    "df_ = df.copy(deep=True)\n",
    "for i in range(len(random_indx_y10)//2):\n",
    "    y = int(random_indx_y10[i])\n",
    "    x = int(random_indx_x10[i])\n",
    "    df_.iat[y, x] =  df_.at[y, x] + s_5\n",
    "\n",
    "df_matrix = np.asmatrix(df_)\n",
    "u, s, vt = np.linalg.svd(df_matrix)\n",
    "\n",
    "s2=[]\n",
    "s2sum=0\n",
    "for i in range(0, s.shape[0]):\n",
    "    s2.append(s[i]**2)\n",
    "    s2sum=s2sum+s[i]**2\n",
    "    \n",
    "s2matr=np.array(s2)    \n",
    "total_variance = 0\n",
    "num_sv = 0\n",
    "for i in range(0, s.shape[0]):\n",
    "    if total_variance < 0.9:\n",
    "        total_variance += s2matr[i]/s2sum\n",
    "        num_sv += 1\n",
    "\n",
    "print(\"With s = 0.5, fraction = 5% \", round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad07a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With s = 1, fraction = 10%  78.60000000000001 % singular values to capture 90 % variance\n"
     ]
    }
   ],
   "source": [
    "df_ = df.copy(deep=True)\n",
    "for i in range(len(random_indx_y10)):\n",
    "    y = int(random_indx_y10[i])\n",
    "    x = int(random_indx_x10[i])\n",
    "    df_.iat[y, x] =  df_.at[y, x] + s_1\n",
    "\n",
    "df_matrix = np.asmatrix(df_)\n",
    "u, s, vt = np.linalg.svd(df_matrix)\n",
    "\n",
    "s2=[]\n",
    "s2sum=0\n",
    "for i in range(0, s.shape[0]):\n",
    "    s2.append(s[i]**2)\n",
    "    s2sum=s2sum+s[i]**2\n",
    "    \n",
    "s2matr=np.array(s2)    \n",
    "total_variance = 0\n",
    "num_sv = 0\n",
    "for i in range(0, s.shape[0]):\n",
    "    if total_variance < 0.9:\n",
    "        total_variance += s2matr[i]/s2sum\n",
    "        num_sv += 1\n",
    "print(\"With s = 1, fraction = 10% \", round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d20eaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With s = 1, fraction = 5%  73.6 % singular values to capture 90 % variance\n"
     ]
    }
   ],
   "source": [
    "df_ = df.copy(deep=True)\n",
    "for i in range(len(random_indx_y10)//2):\n",
    "    y = int(random_indx_y10[i])\n",
    "    x = int(random_indx_x10[i])\n",
    "    df_.iat[y, x] =  df_.at[y, x] + s_1\n",
    "\n",
    "df_matrix = np.asmatrix(df_)\n",
    "u, s, vt = np.linalg.svd(df_matrix)\n",
    "\n",
    "s2=[]\n",
    "s2sum=0\n",
    "for i in range(0, s.shape[0]):\n",
    "    s2.append(s[i]**2)\n",
    "    s2sum=s2sum+s[i]**2\n",
    "    \n",
    "s2matr=np.array(s2)    \n",
    "total_variance = 0\n",
    "num_sv = 0\n",
    "for i in range(0, s.shape[0]):\n",
    "    if total_variance < 0.9:\n",
    "        total_variance += s2matr[i]/s2sum\n",
    "        num_sv += 1\n",
    "\n",
    "print(\"With s = 1, fraction = 5% \", round(num_sv/s.shape[0], 3)*100, \"% singular values to capture\", round(total_variance*100),\"% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "535b175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#так как именно s = 1, fraction = 5% будет использовано в LENS разложении, выведем количество сингулярных значений объясняющих 90% информации \n",
    "num_sv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "035b4800",
   "metadata": {},
   "source": [
    "Чем вводится больше аномалий, или чем они больше по размеру, тем больший процент сингулярных чисел необходим для объяснения 90% вариативности данных."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84fbad0b",
   "metadata": {},
   "source": [
    "### LENS Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ec553e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresh(x, T):\n",
    "    \"\"\"Soft threshold function\"\"\"\n",
    "    return  np.multiply(np.sign(x), np.maximum(np.abs(x) - T, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dccdf70-f655-4bab-a1dc-3c4151476428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADMM(D, frac_anomaly, frac_missing, random_indx_y10_miss, random_indx_x10_miss, m=182, n=3127):\n",
    "\n",
    "    X = np.random.randint(-70, 61, size=(n, n)) \n",
    "    Y = np.random.randint(-70, 61, size=(m, n))\n",
    "    Z = np.random.randint(-70, 61, size=(m, n))   \n",
    "    A = np.array([np.where(x == None, 0, 1) for x in D])\n",
    "    B = np.eye(182)\n",
    "    C = np.eye(182)\n",
    "    X_0 = np.random.randint(-70, 61, size=(n, n))\n",
    "    X_1 = np.random.randint(-70, 61, size=(n, n))\n",
    "    Y_0 = np.random.randint(-70, 61, size=(m, n))\n",
    "    M_0 = np.random.randint(-1, 1, size=(n, n))\n",
    "    M_1 = np.random.randint(-1, 1, size=(n, n))\n",
    "    N = np.random.randint(-1, 1, size=(m, n))\n",
    "    M = np.random.randint(-1, 1, size=(m, n))\n",
    "    W = np.random.randint(-1, 1, size=(m, n))\n",
    "    \n",
    "    #списки индексов ячеек с  не пропущенными значениями\n",
    "    non_elem = np.argwhere(D != None)\n",
    "    D[np.isnan(D)] = 0\n",
    "    E = np.array([[1 for col in range(3127)] for row in range(182)])\n",
    "    for i in range(len(non_elem)):\n",
    "        E[non_elem[i][0]][non_elem[i][1]] = 0\n",
    "        W[non_elem[i][0]][non_elem[i][1]] = 0\n",
    "    shape_d = (m, n)    #(182, 3127)\n",
    "    shape_X = shape_Y = shape_d\n",
    "    size_d = m*n\n",
    "    #nu_D - это доля записей в D, которые не являются ни пропущенными, ни ошибочными.\n",
    "    nu_D = 1 - (frac_missing+frac_anomaly)/size_d\n",
    "\n",
    "    mu = 1.01\n",
    "    p = 1.05\n",
    "    \n",
    "    J = D - A@X_0 - B@Y_0 - W\n",
    "    alpha = (shape_X[0]**(0.5) + shape_X[1]**(0.5))*nu_D\n",
    "    beta = (2*math.log(shape_Y[0]*shape_Y[1]))**(0.5)\n",
    "    gamma = 1\n",
    "    theta = 10\n",
    "    K = 1\n",
    "    P_1 = np.eye(n)\n",
    "    q1_list_1 = [1] +  [0] * (n-1)\n",
    "    q1_list_2 = [1, -1] + [0] * (n-2)\n",
    "    Q_1 =  scipy.linalg.toeplitz(q1_list_1, q1_list_2)\n",
    "    R_1 = np.zeros(n)\n",
    "\n",
    "    #sigma_D определяется на каждой иттерации ADM алгоритма\n",
    "    #как стандартное отклонение ряда значений J[i,j], где E[i,j]=0\n",
    "    tmp_nonzero = []\n",
    "    indices = np.nonzero(~np.isnan(E))\n",
    "    non_elem = np.column_stack(indices)\n",
    "    for i in range(len(non_elem)):\n",
    "        tmp_nonzero.append(J[non_elem[i][0]][non_elem[i][1]]) \n",
    "    sigma_D = statistics.stdev(tmp_nonzero)\n",
    "    sigma = theta*sigma_D\n",
    "    MAX_ITER = 140\n",
    "    for i in range(MAX_ITER):\n",
    "        print(\"Iteration\", i)\n",
    "\n",
    "        #first step X\n",
    "        J = 1/(K+1)*(X_1 + M_1/mu + X_0 + M_0/mu)\n",
    "        t = alpha/(mu*(K+1))        \n",
    "        # QR факторизация\n",
    "        Q, R = np.linalg.qr(J)   \n",
    "        u, s, vt = np.linalg.svd(J, full_matrices=False)\n",
    "        S = np.diag(s)\n",
    "        s = soft_thresh(S, t)\n",
    "        s = np.diag(s)\n",
    "        X = Q.dot(u[:,:2]).dot(np.diag(s[:2])).dot(vt[:2,:])\n",
    "\n",
    "        #second step \n",
    "        J = X - M_1/mu\n",
    "        R = P_1.transpose() @ R_1 @ Q_1 + ((mu*sigma)/gamma) * J\n",
    "        up, sp, vtp = np.linalg.svd(P_1 @ P_1.transpose())\n",
    "        uq, sq, vtq = np.linalg.svd(Q_1 @ Q_1.transpose())\n",
    "        X_1 = up @ np.divide((vtp @ R @ uq), (sp*sq.transpose() + (mu*sigma/gamma))) @ vtq\n",
    "        \n",
    "        #third step with X0\n",
    "        J_0 = X-M_0/mu\n",
    "        J = D-np.matmul(B, Y_0)-np.matmul(C, Z)-W + M/mu\n",
    "        X_0 = np.linalg.inv(A.transpose()@A + np.eye(n))@(A.transpose()@J + J_0)\n",
    "        \n",
    "        #fourth step Y\n",
    "        J = Y_0 + N/mu\n",
    "        t = beta/mu\n",
    "        Y = soft_thresh(J, t)\n",
    "\n",
    "        #fifth step Y_0\n",
    "        J_0 = Y - N/mu\n",
    "        J = D-np.matmul(A, X_0)-np.matmul(C, Z)-W + M/mu\n",
    "        Y_0 = np.linalg.inv(B.transpose()@B + np.eye(m))@(B.transpose()@J + J_0)\n",
    "        \n",
    "        #sixth step Z\n",
    "        J = D-np.matmul(A, X_0)-np.matmul(B, Y_0)-W + M/mu\n",
    "        Z = np.linalg.inv(1/(mu*sigma)*np.eye(m) + np.matmul(C.transpose(), C))@(np.matmul(C.transpose(),J))\n",
    "        \n",
    "        #seventh step W\n",
    "        W = np.multiply(E, (D-np.matmul(A, X_0)-np.matmul(B, Y_0)-np.matmul(C, Z) + M/mu))\n",
    "        \n",
    "        #eighth step sigma\n",
    "        #sigma определяется на каждой иттерации ADM алгоритма, \n",
    "        #как стандартное отклонение ряда значений J[i,j], где E[i,j]=0\n",
    "        theta = 10\n",
    "        J = D-np.matmul(A, X_0)-np.matmul(B, Y_0)-W\n",
    "        tmp_nonzero = []\n",
    "        indices = np.nonzero(~np.isnan(E))\n",
    "        non_elem = np.column_stack(indices)\n",
    "        for i in range(len(non_elem)):\n",
    "            tmp_nonzero.append(J[non_elem[i][0]][non_elem[i][1]])      \n",
    "        \n",
    "        sigma_D = statistics.stdev(tmp_nonzero)           \n",
    "        sigma = theta*sigma_D\n",
    "\n",
    "        #ninth step\n",
    "        #Every itteration update M, M_0, N\n",
    "        M = M + mu * (D-np.matmul(A, X_0)-np.matmul(B, Y_0)-np.matmul(C, Z)-W)\n",
    "        M_0 = M_0 + mu * (X_0-X)\n",
    "        M_1 = M_1 + mu*(X_1-X)\n",
    "        N = N +  mu *  (Y_0-Y)\n",
    "        \n",
    "        #tenth step\n",
    "        #Every itteration update mu\n",
    "        mu = p*mu\n",
    "        #Every 100 iterations, we multiply ρ by 1.05\n",
    "        if isinstance(i//100, int):\n",
    "            p_ = 1.05\n",
    "            p = p*p_\n",
    "        if sigma_D == 0:\n",
    "            return D\n",
    "        D = A @ X_0 + B @ Y_0 + C @ Z + W\n",
    "    D = A @ X_0 + B @ Y_0 + C @ Z + W    \n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ef7b1-740e-4793-b84f-6d594a08941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "df_ = df.copy(deep=True)\n",
    "D = np.asmatrix(df_)\n",
    "tmp_arr = copy.deepcopy(main_arr)\n",
    "frac_anomaly = round(182*3127*0.1)//2\n",
    "#frac_anomaly = 0\n",
    "\n",
    "new_loss_result_list = []\n",
    "new_loss_result_list_2 = []\n",
    "new_loss_result_list_3 = []\n",
    "\n",
    "p_ = [0.2, 0.4, 0.8]\n",
    "anomaly_size = [0.5, 1, 2, 5]\n",
    "anomaly_ratio = [0, 0.05, 0.1, 0.2]\n",
    "loss_probability = [0.2, 0.4, 0.8]\n",
    "loss = 0.2\n",
    "anomaly_r = 0.1\n",
    "anomaly_s = 1\n",
    "\n",
    "#for p in p_:\n",
    "for anomaly_r in anomaly_ratio:\n",
    "#for anomaly_s in anomaly_size:\n",
    "#for loss in loss_probability:\n",
    "    new_nmae_err_list = []\n",
    "    new_nmae_err_list_2 = []\n",
    "    new_nmae_err_list_3 = []\n",
    "    nmae_err_list = []\n",
    "    nmae_err_list_2 = []\n",
    "    nmae_err_list_3 = []\n",
    "    frac_missing = round(182*3127*loss)\n",
    "    for i in range(1):\n",
    "        #first technique\n",
    "        random_indx_y10_miss = np.random.randint(0, 182, size=round(182*3127*loss))\n",
    "        random_indx_x10_miss = np.random.randint(0, 3127, size=round(182*3127*loss))\n",
    "        \n",
    "#         #second technique\n",
    "#         random_indx_y10_miss = np.array([i for i in range(182)])\n",
    "#         random_indx_x10_miss = np.random.randint(0, 3127, size=round(182*3127*loss))\n",
    "        \n",
    "#         #third technique\n",
    "#         random_indx_y10_miss = np.random.randint(0, 182, size=round(182*3127*loss))\n",
    "#         random_indx_x10_miss = np.array([i for i in range(3127)])\n",
    "        \n",
    "#         #third technique\n",
    "#         random_indx_y10_miss = np.random.randint(0, 100, size=round(100*150*loss))\n",
    "#         random_indx_x10_miss = np.random.randint(0, 150, size=round(100*150*p))\n",
    "\n",
    "#         #fourth technique\n",
    "#         random_indx_y10_miss = np.array([i for i in range(100)])\n",
    "#         random_indx_x10_miss = np.random.randint(0, 150, size=round(100*150*loss))\n",
    "        \n",
    "#         #fifth technique\n",
    "#         random_indx_y10_miss = np.random.randint(0, 100, size=round(100*150*loss))\n",
    "#         random_indx_x10_miss = np.array([i for i in range(150)])\n",
    "\n",
    "        list_D = []\n",
    "        list_result_D = []\n",
    "        list_result_D_2 = []\n",
    "        list_result_D_3 = []\n",
    "        tmp_arr = copy.deepcopy(main_arr)\n",
    "        for i in range(len(random_indx_y10_miss)):\n",
    "            y = int(random_indx_y10_miss[i])\n",
    "            x = int(random_indx_x10_miss[i])\n",
    "            list_D.append(main_arr[y][x])\n",
    "            tmp_arr[y][x] = None\n",
    "        \n",
    "        mean = np.nanmean(tmp_arr)\n",
    "        tmp_arr_for_svd = copy.deepcopy(tmp_arr)\n",
    "        tmp_arr_for_svd[np.isnan(tmp_arr_for_svd)] = 0\n",
    "        \n",
    "        indices = np.nonzero(np.isnan(tmp_arr_for_svd)) \n",
    "        random_indx_y10_anomal = np.random.randint(0, 182, size=round(182*3127*0.1))\n",
    "        random_indx_x10_anomal = np.random.randint(0, 3127, size=round(182*3127*0.1))\n",
    "    \n",
    "        for i in range(len(random_indx_y10_anomal)//2):\n",
    "            y = int(random_indx_y10_anomal[i])\n",
    "            x = int(random_indx_x10_anomal[i])\n",
    "            tmp_arr[y][x] += s_1\n",
    "            tmp_arr_for_svd[y][x] += s_1\n",
    "        \n",
    "        U, sigma, V = np.linalg.svd(tmp_arr_for_svd)\n",
    "        # Выберем только первые две компоненты для аппроксимации\n",
    "        U = U[:, :2]\n",
    "        sigma = np.diag(sigma[:2])\n",
    "        V = V[:2, :]\n",
    "        # Вычислим аппроксимированную матрицу\n",
    "        D_approx = U @ sigma @ V\n",
    "\n",
    "        # Применяем K ближайших соседей для локальной интерполяции:\n",
    "        knn = KNeighborsRegressor(n_neighbors=2)\n",
    "        matrix_interp = knn.fit(D_approx, main_arr).predict(D_approx)\n",
    "\n",
    "        # Получаем итоговую матрицу, заменяя пропущенные значения после локальной интерполяции:\n",
    "        D_svd_knn = copy.deepcopy(tmp_arr)\n",
    "        D_svd_knn[np.isnan(D_svd_knn)] = matrix_interp[np.isnan(D_svd_knn)]\n",
    "        \n",
    "        for i in range(len(random_indx_y10_miss)):\n",
    "            y = int(random_indx_y10_miss[i])\n",
    "            x = int(random_indx_x10_miss[i])\n",
    "            list_result_D_3.append(D_svd_knn[y][x])\n",
    "            list_result_D_2.append(D_approx[y][x])\n",
    "           \n",
    "            \n",
    "        m=182\n",
    "        n=3127\n",
    "        result_D = ADMM(tmp_arr, frac_anomaly, frac_missing, random_indx_y10_miss, random_indx_x10_miss, m, n)\n",
    "        for i in range(len(random_indx_y10_miss)):\n",
    "            y = int(random_indx_y10_miss[i])\n",
    "            x = int(random_indx_x10_miss[i])\n",
    "            list_result_D.append(result_D[y][x])         \n",
    "        \n",
    "        raznost = [x - y for x, y in zip(list_D, list_result_D)]\n",
    "        raznost_2 = [x - y for x, y in zip(list_D, list_result_D_2)]\n",
    "        raznost_3 = [x - y for x, y in zip(list_D, list_result_D_3)]\n",
    "        new_nmae_err_list.append(np.mean(np.abs(raznost) / np.nanmean(np.abs(list_D))))\n",
    "        new_nmae_err_list_2.append(np.mean(np.abs(raznost_2) / np.nanmean(np.abs(list_D))))\n",
    "        new_nmae_err_list_3.append(np.mean(np.abs(raznost_3) / np.nanmean(np.abs(list_D))))\n",
    "    \n",
    "    new_loss_result_list.append(np.mean(new_nmae_err_list))\n",
    "    new_loss_result_list_2.append(np.mean(new_nmae_err_list_2))\n",
    "    new_loss_result_list_3.append(np.mean(new_nmae_err_list_3))   \n",
    "\n",
    "print(new_loss_result_list)\n",
    "print(new_loss_result_list_2)\n",
    "print(new_loss_result_list_3)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basic Python Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
